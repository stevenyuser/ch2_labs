{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Football's Coming Home\n",
    "\n",
    "# 1 \n",
    "euro2012 = pd.read_csv(\"https://raw.githubusercontent.com/guipsamora/pandas_exercises/master/02_Filtering_%26_Sorting/Euro12/Euro_2012_stats_TEAM.csv\")\n",
    "\n",
    "# 2\n",
    "len(euro2012[\"Team\"])\n",
    "# 16 teams\n",
    "\n",
    "# 3\n",
    "discipline = euro2012[[\"Team\", \"Yellow Cards\", \"Red Cards\"]]\n",
    "\n",
    "# a\n",
    "discipline.sort_values(by=[\"Red Cards\", \"Yellow Cards\"], ascending = [False, False])\n",
    "\n",
    "# b\n",
    "euro2012[\"Yellow Cards\"].mean\n",
    "\n",
    "# 4 \n",
    "\n",
    "# a\n",
    "euro2012a = euro2012.copy()\n",
    "euro2012a[\"Clearances off line\"].fillna(0, inplace=True)\n",
    "\n",
    "# b \n",
    "euro2012b = euro2012.copy()\n",
    "euro2012b[euro2012b[\"Goals\"] > 6]\n",
    "\n",
    "# c\n",
    "euro2012c = euro2012.copy()\n",
    "euro2012c.iloc[:,1:7]\n",
    "\n",
    "# d\n",
    "euro2012d = euro2012.copy()\n",
    "euro2012d[euro2012d[\"Team\"].isin([\"England\", \"Italy\", \"Russia\"])][\"Shooting Accuracy\"]\n",
    "\n",
    "# 5\n",
    "euro2012[\"% goals-to-target\"] = euro2012[\"Goals\"] / euro2012[\"Shots on target\"]\n",
    "\n",
    "# a \n",
    "# Low goals to on target shots percentage is unlucky, therefore Poland is most unlucky\n",
    "euro2012.sort_values(by=\"% goals-to-target\", ascending=True)\n",
    "\n",
    "# b \n",
    "# High goals to on target shots percentage is lucky, therefore Greece is most lucky\n",
    "euro2012.sort_values(by=\"% goals-to-target\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>deaths</th>\n",
       "      <th>battles</th>\n",
       "      <th>size</th>\n",
       "      <th>veterans</th>\n",
       "      <th>readiness</th>\n",
       "      <th>armored</th>\n",
       "      <th>deserters</th>\n",
       "      <th>% deserters</th>\n",
       "      <th>% losses</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>12.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>12.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>190.416667</td>\n",
       "      <td>8.416667</td>\n",
       "      <td>1127.916667</td>\n",
       "      <td>174.333333</td>\n",
       "      <td>2.083333</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>11.083333</td>\n",
       "      <td>0.011172</td>\n",
       "      <td>0.183930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>227.027615</td>\n",
       "      <td>10.849871</td>\n",
       "      <td>240.241719</td>\n",
       "      <td>280.254214</td>\n",
       "      <td>0.792961</td>\n",
       "      <td>0.514929</td>\n",
       "      <td>12.324833</td>\n",
       "      <td>0.013169</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>25.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>849.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.001429</td>\n",
       "      <td>0.024951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>41.500000</td>\n",
       "      <td>3.750000</td>\n",
       "      <td>983.500000</td>\n",
       "      <td>34.250000</td>\n",
       "      <td>1.750000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.750000</td>\n",
       "      <td>0.001948</td>\n",
       "      <td>0.047088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>62.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1025.500000</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>0.003406</td>\n",
       "      <td>0.077518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>306.250000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>1174.250000</td>\n",
       "      <td>141.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>0.024507</td>\n",
       "      <td>0.287793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>616.000000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>1592.000000</td>\n",
       "      <td>949.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>0.036514</td>\n",
       "      <td>0.554205</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           deaths    battles         size    veterans  readiness    armored  \\\n",
       "count   12.000000  12.000000    12.000000   12.000000  12.000000  12.000000   \n",
       "mean   190.416667   8.416667  1127.916667  174.333333   2.083333   0.583333   \n",
       "std    227.027615  10.849871   240.241719  280.254214   0.792961   0.514929   \n",
       "min     25.000000   2.000000   849.000000    1.000000   1.000000   0.000000   \n",
       "25%     41.500000   3.750000   983.500000   34.250000   1.750000   0.000000   \n",
       "50%     62.000000   6.000000  1025.500000   55.000000   2.000000   1.000000   \n",
       "75%    306.250000   8.000000  1174.250000  141.000000   3.000000   1.000000   \n",
       "max    616.000000  42.000000  1592.000000  949.000000   3.000000   1.000000   \n",
       "\n",
       "       deserters  % deserters   % losses  \n",
       "count  12.000000    12.000000  12.000000  \n",
       "mean   11.083333     0.011172   0.183930  \n",
       "std    12.324833     0.013169   0.200000  \n",
       "min     2.000000     0.001429   0.024951  \n",
       "25%     2.750000     0.001948   0.047088  \n",
       "50%     3.500000     0.003406   0.077518  \n",
       "75%    24.000000     0.024507   0.287793  \n",
       "max    31.000000     0.036514   0.554205  "
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Ten-Hut\n",
    "raw_data = {'regiment': ['Nighthawks', 'Nighthawks', 'Nighthawks', 'Nighthawks', 'Dragoons', 'Dragoons', 'Dragoons', 'Dragoons', 'Scouts', 'Scouts', 'Scouts', 'Scouts'],\n",
    "            'company': ['1st', '1st', '2nd', '2nd', '1st', '1st', '2nd', '2nd','1st', '1st', '2nd', '2nd'],\n",
    "            'deaths': [523, 52, 25, 616, 43, 234, 523, 62, 62, 73, 37, 35],\n",
    "            'battles': [5, 42, 2, 2, 4, 7, 8, 3, 4, 7, 8, 9],\n",
    "            'size': [1045, 957, 1099, 1400, 1592, 1006, 987, 849, 973, 1005, 1099, 1523],\n",
    "            'veterans': [1, 5, 62, 26, 73, 37, 949, 48, 48, 435, 63, 345],\n",
    "            'readiness': [1, 2, 3, 3, 2, 1, 2, 3, 2, 1, 2, 3],\n",
    "            'armored': [1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1],\n",
    "            'deserters': [4, 24, 31, 2, 3, 4, 24, 31, 2, 3, 2, 3],\n",
    "            'origin': ['Arizona', 'California', 'Texas', 'Florida', 'Maine', 'Iowa', 'Alaska', 'Washington', 'Oregon', 'Wyoming', 'Louisana', 'Georgia']}\n",
    "\n",
    "# 1\n",
    "data = pd.DataFrame(raw_data)\n",
    "\n",
    "# 2\n",
    "data.set_index(keys= \"origin\", inplace=True)\n",
    "\n",
    "# 3\n",
    "data.loc[[\"Maine\", \"Alaska\"]][[\"deaths\", \"size\", \"deserters\"]]\n",
    "data.iloc[3:7,3:6]\n",
    "data.iloc[5:]\n",
    "data[(data[\"deaths\"] > 500) | (data[\"deaths\"] < 50)]\n",
    "\n",
    "# 4\n",
    "\n",
    "# Nighthawks 2nd was most loyal because they had the least percentage of deserters\n",
    "data[\"% deserters\"] = data[\"deserters\"] / data[\"size\"]\n",
    "data.sort_values(by=\"% deserters\",ascending=True)\n",
    "\n",
    "# Scouts 2nd was most ready because they had the least percentage of losses\n",
    "data[\"% losses\"] = (data[\"deaths\"] + data[\"deserters\"]) / data[\"size\"]\n",
    "data.sort_values(by=\"% losses\",ascending=True)\n",
    "\n",
    "# For deaths -  Arizona, Alaska, Florida skew the data, their values are more than a std above the mean\n",
    "# For battles - California skews the data, values are more than a std above the mean\n",
    "# For size - no outliers, values are uniform around the mean, close to mean around the std\n",
    "# For veterans - Alasks skews the data, values are more than a std above the mean\n",
    "# For readiness - 3 skews more because mean is closer to 3 than 1\n",
    "# For armored - the data is skewed slightly to 1 groups because the mean is closer to 1\n",
    "# For deserters - Washington,Alaska,Texas,California skew the data because they're outside std of the mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Doing Time\n",
    "\n",
    "# 1\n",
    "crime = pd.read_csv(\"https://raw.githubusercontent.com/guipsamora/pandas_exercises/master/04_Apply/US_Crime_Rates/US_Crime_Rates_1960_2014.csv\", parse_dates=[\"Year\"])\n",
    "crime.set_index(keys= \"Year\", inplace=True)\n",
    "\n",
    "# 2\n",
    "sixties = crime.loc[\"1960-01-01\":\"1969-01-01\", [\"Robbery\", \"Murder\", \"Burglary\"]].sum()\n",
    "seventies = crime.loc[\"1970-01-01\":\"1979-01-01\", [\"Robbery\", \"Murder\", \"Burglary\"]].sum()\n",
    "eighties = crime.loc[\"1980-01-01\":\"1989-01-01\", [\"Robbery\", \"Murder\", \"Burglary\"]].sum()\n",
    "nineties = crime.loc[\"1990-01-01\":\"1999-01-01\", [\"Robbery\", \"Murder\", \"Burglary\"]].sum()\n",
    "hundreds = crime.loc[\"2000-01-01\":\"2009-01-01\", [\"Robbery\", \"Murder\", \"Burglary\"]].sum()\n",
    "tens = crime.loc[\"2010-01-01\":\"2019-01-01\", [\"Robbery\", \"Murder\", \"Burglary\"]].sum()\n",
    "\n",
    "# 3\n",
    "sixties_violent = crime.loc[\"1960-01-01\":\"1969-01-01\", [\"Violent\"]].sum()\n",
    "seventies_violent = crime.loc[\"1970-01-01\":\"1979-01-01\", [\"Violent\"]].sum()\n",
    "eighties_violent = crime.loc[\"1980-01-01\":\"1989-01-01\", [\"Violent\"]].sum()\n",
    "nineties_violent = crime.loc[\"1990-01-01\":\"1999-01-01\", [\"Violent\"]].sum()\n",
    "hundreds_violent = crime.loc[\"2000-01-01\":\"2009-01-01\", [\"Violent\"]].sum()\n",
    "tens_violent = crime.loc[\"2010-01-01\":\"2019-01-01\", [\"Violent\"]].sum()\n",
    "decades_violent = pd.Series(data=[sixties_violent, seventies_violent, eighties_violent, nineties_violent, hundreds_violent, tens_violent], index=[\"Sixties\", \"Seventies\", \"Eighties\", \"Nineties\", \"Hundreds\", \"Tens\"])\n",
    "# 1990s because it has the most violent crime out of all the decades\n",
    "\n",
    "# 4\n",
    "# I was a bit surprised that the 80s had a lower crime rate than the 90s, because the 80s was when there was a crack epidemic, when crack flooded the inner cities, causing great damage to communities.\n",
    "# In addition, I thought that crime fell sharply in the 90s. According to multiple academic papers, crime fell dramatically in the 90s.\n",
    "# However, this can probably be explained because crime was on an upward trend from the sixties and just slowed down in the nineties (as shown by the data).\n",
    "# In addition, the crack epidemic was still going on during the early 90s.\n",
    "decades_violent"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
